# 抓取指导

下面的流程总结了重建「VIPRPG 紅白 2010」时踩过的坑，将其抽象成可复用的步骤。目标是把祭典站点的 HTML/资源抓取到 `catch/` 与 `public/`，并生成结构化的 `src/data/works/<slug>.json`。整个过程分为「线索搜集 → 本地缓存 → 资产校验 → 数据生成 → 复核归档」。

## 预备工作
- 建议使用 `npm`（含 `tsx`）、`curl`、`rg` 等工具，保证命令与脚本可直接运行。
- 在 `catch/` 目录下创建 `<slug>/` 的工作区：存放原始 HTML (`menu_top.html`、`menu_entry.html`、`entry/*.html|.png` 等) 与抓取摘要。
- 凡是从 Wayback Machine 获取的资源，都要记录其快照 URL，确保后续可复现。
- 目标清单：统一在 `catch/festival-urls.json` 维护每个祭典的 `slug`、名称与入口 URL；抓取脚本/手工流程默认以此为来源，不再另行询问 `slug`。
- 进度记录：统一在 `catch/festival-scrape-status.md` 以 `[ ]`/`[x]` 维护未完成/完成状态，必要时在同一行补充简要备注；开始、完成或回退时仅更新该文件即可。

## 通用抓取约束
- 页面抓取默认通过 Jina Reader 代理获取源站与镜像页面，使用 `https://r.jina.ai/<scheme>://<host>/<path>` 前缀；Wayback 页面因被屏蔽，需直连 `web.archive.org`（不经 `r.jina.ai`）。
- Wayback 域名检索：先 GET 请求 `https://web.archive.org/web/timemap/json?url={url}&matchType=prefix&collapse=urlkey&output=json&fl=original%2Cmimetype%2Ctimestamp%2Cendtimestamp%2Cgroupcount%2Cuniqcount&filter=!statuscode%3A%5B45%5D..&limit=10000` 来获取域名下所有存档页面。
- 外部站点可用性：对外部网站链接（如 Shitaraba 讨论版、Google Drive 等）先进行可用性检测（GET/HEAD 返回 2xx 视为可用）。若可用，则在数据中保留原始链接，不替换为 Wayback 快照；仅当原站不可用时才记录 Wayback 链接。
- Wayback 快照选择：
  - HTML 页面优先使用 `fw_` 快照以获得完整的重写链接；图片等二进制资源优先 `im_`，失败再回退 `id_`。
  - 时间戳优先使用 `endtimestamp`（Wayback 归并后的最新组时间），其次 `timestamp`；如仍失败，可尝试 `https://web.archive.org/web/2/{original}` 的“最近快照”重定向。
  - 无扩展名或 `mimetype=unk` 的记录，先在 Timemap 中查找“同名+扩展名”的条目；不要凭空猜测扩展名。确无扩展项时，可按原样尝试 Wayback 直连验证。
- 多源 Timemap 合并：对于存在镜像/备份域名的站点（如 2018 夏祭 `vipkohaku.x.fc2.com/2018s/` 与 `www.geocities.jp/tkoolvip2018summer/`），分别拉取 Timemap 并按作品编号合并 `detail/icon/截图` 候选，按上述快照选择策略依次尝试。
- 目的：去除 Referer、副作用脚本与不稳定重定向，统一超时与缓存，提升可复现性与对比性。
- 二进制资产（图标/截图/压缩包）仍按原始直链获取；若站点存在防盗链，继续采用“移除 Referer”的策略，仅在 `content-type` 以 `image/` 等二进制前缀时写入资产。
- 例：
  - 源站页面 `https://vipkouhaku2024.x.2nt.com/menu_entry.html` → `https://r.jina.ai/https://vipkouhaku2024.x.2nt.com/menu_entry.html`
  - Wayback 页面 `https://web.archive.org/web/20101231235959/http://example.com/entry/1.html` → 直连 `https://web.archive.org/web/20101231235959/http://example.com/entry/1.html`（不要加 `r.jina.ai`）
  - 对经由 `r.jina.ai` 获取的页面内容按快照原样保存至 `catch/` 目录用于复查；Wayback 页面直连获取并同样保存。如遇解析异常确需其他直连，请在 summary 中注明原因与来源。

## 站点链路确认
1. **首页/Banner**：从 Wayback Machine 或现存镜像打开祭典首页（示例：`https://web.archive.org/.../index.html` → `menu_top.html`）。
 - 只使用页面上真实存在的链接，禁止猜测路径。
 - Banner 下载后放入 `public/banners/<slug>.<ext>`，并在 summary 里记录来源快照。
2. **作品列表页**：按照首页导航找到作品列表（例：`index.html` → `index_entry.html` → `menu_entry.html`）。
 - 将页面快照保存到 `catch/menu_entry.html`（默认通过 `https://r.jina.ai/` 拉取；Wayback 直连）。
 - 列表中出现的所有链接、图标路径都要记录，后续按编号逐条抓取。
3. **作品详情页**：列表中的每个链接必须通过语义分析确认指向 `entry/<no>.html`。
 - 每获取一级页面，都要基于页面真实 href 构建下一级 URL。
4. **下载链接**：记录原始下载 URL 供备查，当前阶段不抓取压缩包或镜像到本地；`download` 字段可暂时留空，待后续统一迁移文件时再补全。

### 现代站点补充：Itch.io
- 先抓取 `jam.html`、`entries.html`、`entries-data.json` 等公开页面，并把文件放入 `catch/<slug>/`，避免直接依赖前端动态。
- `entries-data.json` 通常附带作品的 `game.url`、封面与简介，可与列表页的表格信息对照，缺字段时以表格为准。
- Banner 多半出现在正文开头的第一张图片，可在 `jam_content` 中扫描 `img`，挑出 `img.itch.zone` 的原图保存到 `public/banners/<slug>.<ext>` 并写入 summary。
- 截图优先使用 `.screenshot_list` 中 `<a>` 指向的原图；若页面只暴露 `<img>` 缩略图，再回退到缩略图或封面原图，最终只保留明确出现在画廊的资源。
- 记录每个作品的 `download` 来源与 `.../download_url` 接口，后续如需批量归档可据此补全。
- 利用页面底部的 `#entries` Submissions 网格交叉验证列表，按作品标题/作者归一化匹配缺失条目，避免漏抓纯 Itch 上架的作品。
- 遇到同一行中存在多个作品链接（如本篇 + 外传共用编号）时，依次解析每个 `<a>` 并为追加作品生成 `01a/01b` 等后缀编号，同时继承原行的其它元数据。


### 2NT 主站补充（例：VIPRPG紅白2024）
- 站点部署在 `https://vipkouhaku2024.x.2nt.com`；图标与截图位于 `entry/icons/`、`entry/ss/` 等相对路径，必须以索引或详情页的 URL 为基准构建绝对地址，避免手动改写协议或域名。
- 资源直链启用了 2NT 防盗链，只要携带 Referer 就会 302 到 `https://x.2nt.com/jump/?url=...` 并返回 HTML。抓取时务必移除 Referer，必要时附带 `Connection: close` 防止长连接挂起。
- **主催评论字段**：详情页中的「主催コメント」应写入 `hostComment` 字段；`authorComment` 只保留作者自述与 `【備考】`。若脚本输出带 `【主催】`，务必拆分后再写入 JSON。
- 详情页 `<table.entrypage img>` 同时包含主图与补充截图，先排除 `/icons/`、计数器等无关资源，再合并索引页提供的缩略图集合并去重。
- 下载链接多为 Google Drive `id=` 形式；summary 里记录绝对 URL 即可，数据 JSON 的 `download` 字段继续留空，等批量迁移包体时再补。
- 站点响应较慢，完整跑一遍脚本约需 5–8 分钟，建议在稳定网络下一次抓完避免重复请求。
### Atwiki Wiki 补充（例：VIPRPG夏の陣2023）

- 站点离线副本存放在 `[html]23夏24紅白GW/[2023夏(wiki)]`，表格、详情页、附件均可直接从本地 `attach/<page>/<id>/<file>` 读取，抓取脚本无需发起网络请求。
- 作品列表的「タイトル/作者名」列以 `<br>` 分隔，第一段是标题，第二段（若存在）是作者；部分作品未在 wiki 标注作者，数据落地时可以写入 `不明` 并在 summary 里注明来源缺失。
- 详情页的 `h4` 结构只保留「ジャンル」「配信/動画」「DL」「感想掲示板」，作者信息缺失时需回退到索引数据；`div.attach` 同时挂载 32px 图标与全尺寸截图，可通过尺寸阈值（如 <100px 判定为图标）过滤后再写入 `/screenshots/`。
- `DL` 链接有时指向 atwiki 站内的聚合页（例如 `?page=DL`），同样按原样记录到 `downloadSource`，后续人工补链。



### FC2 Entry 补充（例：VIPRPG2023紅白）
- 作品详情按 `entry/<no>.html` 划分，本地 `entry/img/` 同时放置图标与截图（命名为 `i01.*`、`SS01_*.*`）。解析时直接遍历这些 HTML，不依赖首页的脚本生成列表。
- `<h2>` 中的方括号附带使用ツール，可作为 `engine`；`<h3>` 首段包含作者与ジャンル，`<br>` 后面的 `配信/投稿` 字段原样保留。
- 下载按钮集中在 `.e_btn a.btn_dlvote`，可能同时提供日/英版等多个链接，建议信息写入 summary 供人工确认。
- 轮播区域 `.entry_carousel_img img` 即大图截图；小尺寸图标（≤100px）需要过滤，避免混入 `/screenshots/`。

### Geocities 镜像补充（例：VIPRPG 2018夏祭り）
- 已知镜像：`https://www.geocities.jp/tkoolvip2018summer/`，与 FC2 主站 `https://vipkohaku.x.fc2.com/2018s/` 内容相近但收录更完整。
- Timemap 合并：分别拉取两域名的 Timemap，按作品编号（如 `entry01` → `01`）合并 `detail/icon/ss` 候选；同一资源优先选择 `endtimestamp` 更新的快照。
- 路径模式：
  - 详情：`/entryNN.html`
  - 图标：`/entryNNicon.(png|jpg|gif)`，偶见变体如 `entry11icon1.png`
  - 截图：`/entryNNss(.png|.jpg)`，或 `entryNNss1/ss2`（无扩展的条目通常在 Timemap 里另有带扩展名的同名记录）
- 快照类型：HTML 用 `fw_`；图片优先 `im_`，失败回退 `id_`。若两端快照均不可用，而原始直链仍可访问，可按“移除 Referer”的二进制直链策略兜底。
- 列表与详情解析：
  - 列表页作者通常与标题共用一格（“作品名/作者”），通过 `<br>` 分隔，作者在 `<span class="font75">` 中；解析时对整个单元格去标签+分行，不要只读取 `<a>` 的 HTML。
  - 详情页评论通过“作者コメント/作者のコメント/備考”与“管理人コメント/主催コメント”行提取，值在标签后的多行内容（用 `<br>` 拼接换行）。`作者：` 行用于作者名回填，不计入作者评论。

## 本地缓存策略
- HTML 快照统一命名为 `catch/entry/<no>_<label>.html`，`label` 使用来源 URL 的简写（脚本已有 `labelFromUrl` 逻辑可复用）。
- 若表格缺失但 Submissions 中存在作品，至少缓存对应的 Itch 页面 HTML，必要时人工补入列表数据，保持数据源可追溯。
- 原始截图/Icon 也可缓存于 `catch/entry/` 用于排查；最终入库的图标与截图分别写入 `public/icons/<slug>/`、`public/screenshots/<slug>/`。
- 若提前手动下载资源，请保留原始文件，脚本会检测已有文件避免重复请求。

## 脚本运行与核心逻辑
运行示例脚本：
```bash
npx tsx scripts/scrape-2010-kouhaku.ts
```
该脚本现已具备以下工程化保障：
- **资源判定**：`bufferLooksLikeHtml` 与长度校验可以过滤 HTML 响应、空文件，避免把错误页面写进资产目录。
- **尺寸过滤**：`getImageDimensions` 覆盖 PNG/JPG/GIF/BMP，宽高同时 <100px 判定为图标或按钮直接跳过。
- **多分辨率合并**：对同一图片的缩略/原图进行归一，优先保留 `/original/`，每个作品最多写入 `MAX_SCREENSHOTS`（默认 6）张。
- **去重策略**：对通过尺寸校验的截图计算 MD5；内容重复记录为 `duplicate` 并跳过。
- **自动清理**：写入前调用 `purgeEntryScreenshots(no)` 删除旧文件，确保目录只保留本次抓取。
- **结果摘要**：`catch/<slug>-scrape-summary.json` 汇总缺失、跳过原因与下载来源，便于复查。
- **下载包暂缓**：脚本只记录下载来源 (`downloadSource`)，`download` 字段待后续统一迁移文件时再补充。

## 结果复核
1. **截图检查**：
 - 执行 `Get-ChildItem public/screenshots/<slug> | Where-Object { $_.Length -eq 0 }`，确认无 0 字节文件。
 - spot-check 几个条目确保截图确实来自详情页而不是图标，并确认单张作品没有“原图 + 缩略图”的组合。
 - 对照 summary 中 `screenshotReport.saved` 数量，确认与 `public/screenshots/<slug>/` 实际文件数一致。
2. **数据校验**：
 - `npm run validate:data` 确认 JSON 结构合规。
 - `npm run lint` 保障脚本与项目代码风格一致。
3. **比对摘要**：阅读 `catch/<slug>-scrape-summary.json`，关注 `note`、`skippedScreenshots`、`downloadSource` 等字段，必要时回溯对应源页面补抓。


## 常见问题与排查
- **快照缺失**：summary 中的 `failed:<url>` 表示所有候选都失败，可以尝试其他时间戳或寻找镜像站。务必把失败记录留在 summary 里。
- **2NT 防盗链**：若下载到的文件开头是 `<!DOCTYPE html>` 或 summary 出现大量 302，说明请求仍携带 Referer。移除 Referer 并改用逐条 `fetchBinary`，只在响应 `content-type` 前缀为 `image/` 时写入资产。
- **混入图标**：若 summary 有大量 `small:` 项，说明详情页的结构特殊，需要手动剔除或调整阈值（默认 100px）。
- **缺少图标列**：当祭典大部分作品无独立图标时，不必伪造资源；直接在 `src/data/festivals.json` 将对应 `columns` 中的 `icon` 移除。
- **重复截图**：`duplicate:` 表示同一作品只留下一张有效截图，此为预期行为。
- **封面退化**：若作品没有画廊链接，只能使用封面或缩略图；确认 `collectScreenshotCandidates` 在捕获 `<a>` + 原图时不会回退到缩略图。
- **站点结构差异**：如遇非表格布局，先更新脚本的解析逻辑（例如改写 `collectScreenshotCandidates`），再重新抓取并复核。


## 交付清单与版本管理
- `src/data/works/<slug>.json`：排序稳定、引用到 `/icons/...`、`/screenshots/...` 的相对路径，且已通过验证命令；若祭典未展示图标列，则不要构造虚假图标路径。
- 详情禁用标记：若某作品 `hostComment`、`authorComment`、`ss` 均为空（或缺失，视为抓取缺失），为该作品对象添加 `"detailDisabled": true`，用于在 UI 禁用详情面板。
- `public/banners|icons|screenshots/<slug>/`：资源齐全且与 JSON 对应。
- `catch/<slug>-scrape-summary.json`：保留抓取时的上下文与异常说明。
- `catch/` 下的原始 HTML/快照：用于复查与将来脚本迭代。
- 更新 `catch/festival-scrape-status.md` 标记进度，必要时附上本次抓取的改动说明。

### 多时点快照遍历实践（重要）

- 适用情形：某些时间点的快照内容不完整（无评论/无截图/链接 404），或不同镜像域名（FC2/Geocities 等）保存的内容互有缺失。为最大化还原，应对“详情页、图标、截图”等分别遍历所有可用时间快照，直到命中有效内容为止。

- 候选构建与排序：
  - 为每个站点 Origin 拉取 Timemap（FC2 + 各镜像域名），合并后按资源类型筛选：
    - 详情页：`/(2016GW|viprpg2016gw|...)\/entry(\d{2,3})\.html$`
    - 图标（示例 2016）：`/(2016GW|viprpg2016gw)\/mate\/i0*(\d{1,3})\.(png|jpe?g|gif|bmp)$`
    - 截图（示例 2016）：仅接受 `mate/ss*` 路径下的图片；其他年份按站点结构收敛（示例 2017 紅白：`/img/games/<id>-1.<ext>`、`-2.<ext>`）
  - 按 `endtimestamp`（其次 `timestamp`）降序去重，生成候选清单。

- 访问策略与终止条件：
  - HTML（详情页）：优先 `fw_`，失败回退到 `https://web.archive.org/web/2/{original}`。当成功解析到“作者/主催评论”或“至少 1 张截图”即视为命中，停止继续遍历。
  - 图片（图标/截图）：优先 `im_`，失败回退 `id_`，再回退 `/web/2/`。当图片魔数检查通过且满足约束即命中；截图最多保存 6 张、去重（md5）。
  - 过滤与约束：
    - 跳过计数器/广告/外站壳资源（如 `counter_img.php`、`asumi.shinobi.jp`、`yimg.jp` 等），避免把“Yahoo! JAPAN”标题条等误当截图。
    - 图标排除规则：忽略 `mate/iNN.*` 之外的非作品图标路径；截图仅在约定目录（如 2016 的 `mate/ss*`）内采集。
    - 小图跳过：宽高均 < 100px 的图片（截图）视为非内容性小图，加入 `skipped` 报告。

- 多源合并与优先级：
  - 同一编号在多个域名中均可取候选，统一放入同一清单，按时间排序整体遍历；谁先命中即采用其内容。
  - 典型组合：`vipkohaku.x.fc2.com/2016GW/` 与 `www.geocities.jp/viprpg2016gw/`；不同年份请按实际域名扩展正则。

- 解码与解析健壮性：
  - HTML 需根据 `Content-Type` 与 `<meta charset>` 识别编码；遇到 `Shift_JIS/Windows-31J/MS932` 时用相应编码解码（不要强行 UTF‑8）。
  - 解析评论时支持多写法标签（如“作者コメント/作者から一言”、“管理人コメント/主催コメント”），去除前缀“【】”“：”与全角/半角空格。

- 路径与扩展名：
  - Wayback 回放 URL 可能带查询串（如 `?1216`），保存本地文件时应保留原扩展名（`.png/.jpg/...`），忽略查询参数。
  - 解析相对路径时，以“快照页面自身 URL”为 base 解析，不使用 `<base href>` 指向的无效域名。

- 缓存与审计：
  - 被采用的快照 HTML 至少保存一份到 `catch/<slug>/`（如 `entryNN.html`）。若需保留多时点结果用于对比，建议加上时间戳命名：`entryNN-<timestamp>.html`。
  - 摘要 `catch/<slug>/<slug>-scrape-summary.json` 中写入：尝试过的 `sourcesTried`、图标/截图的 `saved/skipped/failures` 与失败原因，便于二次复查。

- 失败重试与退避：
  - 单资源每步至多重试 2～3 次（指数退避 300ms 起），避免网络偶发错误造成误判。
  - 所有候选均失败时，保留失败列表并在 summary 中显式标注，后续可人工补。

### Wayback CDX 接口用法（逐条资源穷举快照）

- 目的：当 Timemap 合并仍无法命中某一“具体 URL”的完整快照时（例如 `entryNN.html`、`mate/iNN.png`、`img/games/<id>-1.png`），使用 CDX 接口枚举该 URL 的“所有时间快照”，再逐一回放尝试，尽可能补齐评论、图标与截图。

- 接口与常用参数：
  - 基本形式：`http://web.archive.org/cdx/search/cdx?url={original}`
  - 推荐参数：
    - `output=json` —— 机器可读
    - `fl=timestamp,original,statuscode` —— 仅取需要字段（可按需加 `mimetype,digest`）
    - `filter=statuscode:200` —— 仅保留可回放快照
    - 可选：`matchType=exact`（精确匹配）或 `matchType=prefix`（前缀匹配）
  - 示例：
    - 详情页（FC2）：
      `http://web.archive.org/cdx/search/cdx?url=https://vipkohaku.x.fc2.com/2016GW/entry15.html&output=json&fl=timestamp,original,statuscode&filter=statuscode:200`
    - 详情页（Geocities）带端口：
      `http://web.archive.org/cdx/search/cdx?url=http://www.geocities.jp:80/viprpg2016gw/entry48.html&output=json&fl=timestamp,original,statuscode&filter=statuscode:200`

- 回放 URL 组装：
  - HTML：`https://web.archive.org/web/{timestamp}fw_/{original}`（失败退 `https://web.archive.org/web/2/{original}`）
  - 图片：优先 `im_`，失败退 `id_`，再退 `https://web.archive.org/web/2/{original}`
  - 代码模板：`fw_`/`im_`/`id_` 均为 `{timestamp}{kind}_/{original}` 形式

- 结果遍历与终止：
  - 对同一 URL 的 CDX 结果按 `timestamp` 降序遍历；命中目标字段（如作者/主办评论、至少 1 张有效截图、或图标写入成功）即可终止该 URL 的继续遍历。
  - 对同一编号的多域名 URL（FC2/Geocities/不同协议或端口）依次遍历；先命中的先用。

- 解析与过滤注意：
  - 仅接受“站内约定目录”的截图（例：2016 年 `mate/ss*`；2017 紅白 `/img/games/<id>-1.<ext>`…），拒绝计数器/广告/外站壳资源（如 `counter_img.php`、`yimg.jp`、`asumi.shinobi.jp` 等）。
  - 图片保存前应做魔数校验与小图阈值（<100px）过滤；本地文件名保留原扩展名，忽略查询参数（如 `?1216`）。
  - HTML 解码需根据 `Content-Type`/`<meta charset>` 自动识别（`Shift_JIS/Windows-31J/MS932/UTF-8` 等）。

- Host 变体与穷举：
  - 某些老站的 CDX 仅记录了 `:80` 端口或 http 版本，建议同时尝试：
    - `http://example/…`、`https://example/…`、`http://example:80/…`
  - 对同一路径大小写差异、带/不带扩展名（历史站常见）也可按需追加候选。

- 审计与缓存：
  - 为命中的快照页面在 `catch/<slug>/` 生成 `entryNN-<timestamp>.html` 便于复查。
  - 在 `<slug>-scrape-summary.json` 中记录 `detailSourcesTried/iconSourcesTried` 与截图候选计数，方便定位仍缺项。

按照以上步骤，可以稳定复刻 2010 红白的流程，并快速迁移到其他祭典站点。
