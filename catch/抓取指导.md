# 抓取指导

下面的流程总结了重建「VIPRPG 紅白 2010」时踩过的坑，将其抽象成可复用的步骤。目标是把祭典站点的 HTML/资源抓取到 `catch/` 与 `public/`，并生成结构化的 `src/data/works/<slug>.json`。整个过程分为「线索搜集 → 本地缓存 → 资产校验 → 数据生成 → 复核归档」。

## 预备工作
- 建议使用 `npm`（含 `tsx`）、`curl`、`rg` 等工具，保证命令与脚本可直接运行。
- 在 `catch/` 目录下创建 `<slug>/` 的工作区：存放原始 HTML (`menu_top.html`、`menu_entry.html`、`entry/*.html|.png` 等) 与抓取摘要。
- 凡是从 Wayback Machine 获取的资源，都要记录其快照 URL，确保后续可复现。

## 站点链路确认
1. **首页/Banner**：从 Wayback Machine 或现存镜像打开祭典首页（示例：`https://web.archive.org/.../index.html` → `menu_top.html`）。
   - 只使用页面上真实存在的链接，禁止猜测路径。
   - Banner 下载后放入 `public/banners/<slug>.<ext>`，并在 summary 里记录来源快照。
2. **作品列表页**：按照首页导航找到作品列表（例：`index.html` → `index_entry.html` → `menu_entry.html`）。
   - 将 HTML 原样保存到 `catch/menu_entry.html`。
   - 列表中出现的所有链接、图标路径都要记录，后续按编号逐条抓取。
3. **作品详情页**：列表中的每个链接必须通过语义分析确认指向 `entry/<no>.html`。
   - 每获取一级页面，都要基于页面真实 href 构建下一级 URL。
   - 若直接站点访问失败，按 order：原站 → `web.archive.org/...fw_` → `...if_` → `...id_`。抓取脚本会自动尝试多种快照与 http/https 变体。
4. **下载链接**：记录原始下载 URL 供备查，当前阶段不抓取压缩包或镜像到本地；`download` 字段可暂时留空，待后续统一迁移文件时再补全。

### 现代站点补充：Itch.io
- 先抓取 `jam.html`、`entries.html`、`entries-data.json` 等公开页面，并把文件放入 `catch/<slug>/`，避免直接依赖前端动态。
- `entries-data.json` 通常附带作品的 `game.url`、封面与简介，可与列表页的表格信息对照，缺字段时以表格为准。
- Banner 多半出现在正文开头的第一张图片，可在 `jam_content` 中扫描 `img`，挑出 `img.itch.zone` 的原图保存到 `public/banners/<slug>.<ext>` 并写入 summary。
- 截图优先使用 `.screenshot_list` 中 `<a>` 指向的原图；若页面只暴露 `<img>` 缩略图，再回退到缩略图或封面原图，最终只保留明确出现在画廊的资源。
- 记录每个作品的 `download` 来源与 `.../download_url` 接口，后续如需批量归档可据此补全。
- 利用页面底部的 `#entries` Submissions 网格交叉验证列表，按作品标题/作者归一化匹配缺失条目，避免漏抓纯 Itch 上架的作品。
- 遇到同一行中存在多个作品链接（如本篇 + 外传共用编号）时，依次解析每个 `<a>` 并为追加作品生成 `01a/01b` 等后缀编号，同时继承原行的其它元数据。

## 本地缓存策略
- HTML 快照统一命名为 `catch/entry/<no>_<label>.html`，`label` 使用来源 URL 的简写（脚本已有 `labelFromUrl` 逻辑可复用）。
- 若表格缺失但 Submissions 中存在作品，至少缓存对应的 Itch 页面 HTML，必要时人工补入列表数据，保持数据源可追溯。
- 原始截图/Icon 也可缓存于 `catch/entry/` 用于排查；最终入库的图标与截图分别写入 `public/icons/<slug>/`、`public/screenshots/<slug>/`。
- 若提前手动下载资源，请保留原始文件，脚本会检测已有文件避免重复请求。

## 脚本运行与核心逻辑
运行示例脚本：
```bash
npx tsx scripts/scrape-2010-kouhaku.ts
```
该脚本现已具备以下工程化保障：
- **多源重试**：对每个入口/截图依次尝试原站与多份快照，`fetchSnapshotTimestamps` 会自动补齐候选列表。
- **资源判定**：`bufferLooksLikeHtml` 与长度校验可以过滤 HTML 响应、空文件，避免把错误页面写进资产目录。
- **尺寸过滤**：`getImageDimensions` 覆盖 PNG/JPG/GIF/BMP，宽高同时 <100px 判定为图标或按钮直接跳过。
- **多分辨率合并**：对同一图片的缩略/原图进行归一，优先保留 `/original/`，每个作品最多写入 `MAX_SCREENSHOTS`（默认 6）张。
- **去重策略**：对通过尺寸校验的截图计算 MD5；内容重复记录为 `duplicate` 并跳过。
- **自动清理**：写入前调用 `purgeEntryScreenshots(no)` 删除旧文件，确保目录只保留本次抓取。
- **结果摘要**：`catch/<slug>-scrape-summary.json` 汇总缺失、跳过原因与下载来源，便于复查。
- **下载包暂缓**：脚本只记录下载来源 (`downloadSource`)，`download` 字段待后续统一迁移文件时再补充。

## 结果复核
1. **截图检查**：
   - 执行 `Get-ChildItem public/screenshots/<slug> | Where-Object { $_.Length -eq 0 }`，确认无 0 字节文件。
   - spot-check 几个条目确保截图确实来自详情页而不是图标，并确认单张作品没有“原图 + 缩略图”的组合。
   - 对照 summary 中 `screenshotReport.saved` 数量，确认与 `public/screenshots/<slug>/` 实际文件数一致。
2. **数据校验**：
   - `npm run validate:data` 确认 JSON 结构合规。
   - `npm run lint` 保障脚本与项目代码风格一致。
3. **比对摘要**：阅读 `catch/<slug>-scrape-summary.json`，关注 `note`、`skippedScreenshots`、`downloadSource` 等字段，必要时回溯对应源页面补抓。


## 常见问题与排查
- **快照缺失**：summary 中的 `failed:<url>` 表示所有候选都失败，可以尝试其他时间戳或寻找镜像站。务必把失败记录留在 summary 里。
- **混入图标**：若 summary 有大量 `small:` 项，说明详情页的结构特殊，需要手动剔除或调整阈值（默认 100px）。
- **缺少图标列**：当祭典大部分作品无独立图标时，不必伪造资源；直接在 `src/data/festivals.json` 将对应 `columns` 中的 `icon` 移除。
- **重复截图**：`duplicate:` 表示同一作品只留下一张有效截图，此为预期行为。
- **封面退化**：若作品没有画廊链接，只能使用封面或缩略图；确认 `collectScreenshotCandidates` 在捕获 `<a>` + 原图时不会回退到缩略图。
- **站点结构差异**：如遇非表格布局，先更新脚本的解析逻辑（例如改写 `collectScreenshotCandidates`），再重新抓取并复核。


## 交付清单与版本管理
- `src/data/works/<slug>.json`：排序稳定、引用到 `/icons/...`、`/screenshots/...` 的相对路径，且已通过验证命令；若祭典未展示图标列，则不要构造虚假图标路径。
- `public/banners|icons|screenshots/<slug>/`：资源齐全且与 JSON 对应。
- `catch/<slug>-scrape-summary.json`：保留抓取时的上下文与异常说明。
- `catch/` 下的原始 HTML/快照：用于复查与将来脚本迭代。
- 更新 `catch/festival-scrape-status.md` 标记进度，必要时附上本次抓取的改动说明。

按照以上步骤，可以稳定复刻 2010 红白的流程，并快速迁移到其他祭典站点。

